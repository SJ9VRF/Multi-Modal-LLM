# Multi-Modal-LLM
# LLaVa Multimodal Inference

This repository provides an implementation for performing multimodal inference using LLaVa, a multimodal LLM that extends large language models like LLaMa with visual inputs. This project demonstrates how to use LLaVa for generating text based on image inputs.





![Screenshot_2024-08-12_at_10 14 11_AM-removebg-preview](https://github.com/user-attachments/assets/3ddf7246-0b84-461f-8b2c-76b82bafe6db)


## Setup

To set up the environment, run the following command:

```bash
./setup_environment.sh
```



Alternatively, you can manually install the required packages:
```bash
pip install -r requirements.txt
```

## Usage
Open inference_with_llava.ipynb in Jupyter Notebook or Google Colab to run the example inference pipeline.

## References

- [Original LLaVa Paper](https://arxiv.org/abs/2304.08485)
- [LLaVa 1.5 Paper](https://arxiv.org/pdf/2310.03744.pdf)
- [Transformers Documentation](https://huggingface.co/docs/transformers/main/en/model_doc/llava)
